{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c6e6a6-d2af-4bb2-9adc-195a71502495",
   "metadata": {},
   "source": [
    "# Signature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74c380-3185-4464-92be-3c132eca50b9",
   "metadata": {},
   "source": [
    "## Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1955493-018e-4a83-acbb-584579decca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587e5ef-dd2b-4acf-8218-1d087a307c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"G:\\Users\\GDriveAkshatRaj\\Client Projects\\Signature Detection\\handwritten signatures\\sample_Signature\"\n",
    "all_image = r\"G:\\Users\\GDriveAkshatRaj\\Client Projects\\Signature Detection\\handwritten signatures\\Dataset_Signature_Final\\Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfaff52-04dd-45ed-8cc9-ddc2cb26c732",
   "metadata": {},
   "source": [
    "## Laoding the dataset\n",
    "\n",
    "`Use`\n",
    "```\n",
    "torchvision.datasets.ImageFolder\n",
    "```\n",
    "\n",
    "To preprocess the Image we used\n",
    "```\n",
    "from torchvision import  transforms\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d57c1e-612d-49a8-9794-04e360d4a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=all_image,\n",
    "                              transform=transform,)\n",
    "dataloader= DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f1375-2bbf-4acd-9bb8-9c0f1db8701c",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47b56df-0819-48f6-92f8-b5988ae1a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignatureCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 512)\n",
    "        # we have 2 classes genuine and forge\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        # Flatten the Tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c63bbb-d6ff-4099-b2bd-4eac2e7ba999",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- for training a model we'll use loss and optimizer\n",
    "- Loss :- `Cross-Entropy`\n",
    "- Optmizer :- `Adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05fe3cf3-5521-48ec-b659-4032e07c17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss:0.0695\n",
      "Epoch [1/10], Loss:1.3940\n",
      "Epoch [1/10], Loss:1.5909\n",
      "Epoch [1/10], Loss:1.8199\n",
      "Epoch [1/10], Loss:1.8812\n",
      "Epoch [1/10], Loss:2.1073\n",
      "Epoch [1/10], Loss:2.2076\n",
      "Epoch [1/10], Loss:2.3179\n",
      "Epoch [1/10], Loss:2.4203\n",
      "Epoch [1/10], Loss:2.4925\n",
      "Epoch [2/10], Loss:0.0643\n",
      "Epoch [2/10], Loss:0.1289\n",
      "Epoch [2/10], Loss:0.1924\n",
      "Epoch [2/10], Loss:0.2580\n",
      "Epoch [2/10], Loss:0.3282\n",
      "Epoch [2/10], Loss:0.3949\n",
      "Epoch [2/10], Loss:0.4642\n",
      "Epoch [2/10], Loss:0.5296\n",
      "Epoch [2/10], Loss:0.5967\n",
      "Epoch [2/10], Loss:0.6679\n",
      "Epoch [3/10], Loss:0.0662\n",
      "Epoch [3/10], Loss:0.1290\n",
      "Epoch [3/10], Loss:0.1927\n",
      "Epoch [3/10], Loss:0.2576\n",
      "Epoch [3/10], Loss:0.3188\n",
      "Epoch [3/10], Loss:0.3808\n",
      "Epoch [3/10], Loss:0.4366\n",
      "Epoch [3/10], Loss:0.4917\n",
      "Epoch [3/10], Loss:0.5548\n",
      "Epoch [3/10], Loss:0.6095\n",
      "Epoch [4/10], Loss:0.0465\n",
      "Epoch [4/10], Loss:0.1079\n",
      "Epoch [4/10], Loss:0.1614\n",
      "Epoch [4/10], Loss:0.2069\n",
      "Epoch [4/10], Loss:0.2686\n",
      "Epoch [4/10], Loss:0.3265\n",
      "Epoch [4/10], Loss:0.3714\n",
      "Epoch [4/10], Loss:0.4145\n",
      "Epoch [4/10], Loss:0.4751\n",
      "Epoch [4/10], Loss:0.5210\n",
      "Epoch [5/10], Loss:0.0535\n",
      "Epoch [5/10], Loss:0.0944\n",
      "Epoch [5/10], Loss:0.1351\n",
      "Epoch [5/10], Loss:0.1754\n",
      "Epoch [5/10], Loss:0.2182\n",
      "Epoch [5/10], Loss:0.2583\n",
      "Epoch [5/10], Loss:0.3062\n",
      "Epoch [5/10], Loss:0.3371\n",
      "Epoch [5/10], Loss:0.3827\n",
      "Epoch [5/10], Loss:0.4156\n",
      "Epoch [6/10], Loss:0.0381\n",
      "Epoch [6/10], Loss:0.0822\n",
      "Epoch [6/10], Loss:0.1168\n",
      "Epoch [6/10], Loss:0.1695\n",
      "Epoch [6/10], Loss:0.1935\n",
      "Epoch [6/10], Loss:0.2245\n",
      "Epoch [6/10], Loss:0.2511\n",
      "Epoch [6/10], Loss:0.2864\n",
      "Epoch [6/10], Loss:0.3208\n",
      "Epoch [6/10], Loss:0.3401\n",
      "Epoch [7/10], Loss:0.0291\n",
      "Epoch [7/10], Loss:0.0489\n",
      "Epoch [7/10], Loss:0.0774\n",
      "Epoch [7/10], Loss:0.1011\n",
      "Epoch [7/10], Loss:0.1299\n",
      "Epoch [7/10], Loss:0.1622\n",
      "Epoch [7/10], Loss:0.1982\n",
      "Epoch [7/10], Loss:0.2197\n",
      "Epoch [7/10], Loss:0.2363\n",
      "Epoch [7/10], Loss:0.2466\n",
      "Epoch [8/10], Loss:0.0192\n",
      "Epoch [8/10], Loss:0.0327\n",
      "Epoch [8/10], Loss:0.0592\n",
      "Epoch [8/10], Loss:0.0804\n",
      "Epoch [8/10], Loss:0.1065\n",
      "Epoch [8/10], Loss:0.1342\n",
      "Epoch [8/10], Loss:0.1609\n",
      "Epoch [8/10], Loss:0.1832\n",
      "Epoch [8/10], Loss:0.1998\n",
      "Epoch [8/10], Loss:0.2088\n",
      "Epoch [9/10], Loss:0.0122\n",
      "Epoch [9/10], Loss:0.0400\n",
      "Epoch [9/10], Loss:0.0491\n",
      "Epoch [9/10], Loss:0.0637\n",
      "Epoch [9/10], Loss:0.0716\n",
      "Epoch [9/10], Loss:0.0850\n",
      "Epoch [9/10], Loss:0.1003\n",
      "Epoch [9/10], Loss:0.1085\n",
      "Epoch [9/10], Loss:0.1326\n",
      "Epoch [9/10], Loss:0.1369\n",
      "Epoch [10/10], Loss:0.0267\n",
      "Epoch [10/10], Loss:0.0298\n",
      "Epoch [10/10], Loss:0.0359\n",
      "Epoch [10/10], Loss:0.0476\n",
      "Epoch [10/10], Loss:0.0563\n",
      "Epoch [10/10], Loss:0.0685\n",
      "Epoch [10/10], Loss:0.0738\n",
      "Epoch [10/10], Loss:0.0784\n",
      "Epoch [10/10], Loss:0.0861\n",
      "Epoch [10/10], Loss:0.0964\n"
     ]
    }
   ],
   "source": [
    "model = SignatureCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss:{running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc1c2e-a6b8-43c5-bf2b-6b2cb1d47240",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9ad7f1-520e-4c6c-afc8-287dc0f12be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.33333333333333\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct /total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639d3c8-96af-4da8-a0f7-99f0c3306545",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "- So that we can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809b8ef4-a0d0-4a8e-a94b-2cff9e3423ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_signature_detection.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74936ba1-9edd-4cef-afd4-60597aeb28b1",
   "metadata": {},
   "source": [
    "## Testing on Individual Image\n",
    "- Load the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91774fa9-89e1-4229-8481-56f5a41cfacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'handwritten signatures/02102007.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhandwritten signatures/02102007.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m image\n",
      "File \u001b[1;32mG:\\Users\\GDriveAkshatRaj\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'handwritten signatures/02102007.png'"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"handwritten signatures/02102007.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977757e9-d5bf-46a5-99ed-5841036a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(path):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "    # Load the image\n",
    "    image = Image.open(path)\n",
    "    # Apply preprocessing\n",
    "    image = transform(image)\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "image = load_and_preprocess(\"handwritten signatures/real.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af955ed3-56dc-4cb0-a675-771d91ba601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('handwritten signatures/real.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39152b4a-b145-4574-b244-ebeb1bc8e44d",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457dadb-ef90-40de-9ab7-e0214b6c1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(path):\n",
    "    image = load_and_preprocess(path)\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    classes = ['forged', 'genuine']\n",
    "    return classes[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1ccd0-95fe-4353-b882-1f456eb16115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "file = \"handwritten signatures/*.png\"\n",
    "data = glob(file)\n",
    "print(data)\n",
    "\n",
    "for signature in data:\n",
    "    img = Image.open(signature)\n",
    "    img\n",
    "    print(signature.split(\"\\\\\")[-1].split(\".\")[0], make_prediction(signature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc641466-f6a7-423c-8558-855c89e2128f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
